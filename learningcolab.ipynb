{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdfc406",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.00256,
     "end_time": "2026-01-28T17:02:45.459040",
     "exception": false,
     "start_time": "2026-01-28T17:02:45.456480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bharggav/Learning/blob/release1.1/learningcolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11eb530e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:02:45.463654Z",
     "iopub.status.busy": "2026-01-28T17:02:45.463413Z",
     "iopub.status.idle": "2026-01-28T17:02:58.649502Z",
     "shell.execute_reply": "2026-01-28T17:02:58.648860Z"
    },
    "id": "1_Oj4kqybESl",
    "outputId": "051fabf4-d1e4-477c-f1be-bc8a45da6657",
    "papermill": {
     "duration": 13.190756,
     "end_time": "2026-01-28T17:02:58.651463",
     "exception": false,
     "start_time": "2026-01-28T17:02:45.460707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/28 17:02:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|Bharggav| 27|\n",
      "|  2|  harika| 25|\n",
      "|  3|   Nitya| 27|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"sample data frame\").getOrCreate()\n",
    "a=[(1,\"Bharggav\",\"27\"),(2,\"harika\",\"25\"),(3,\"Nitya\",\"27\")]\n",
    "df=spark.createDataFrame(a,[\"id\",\"name\",\"age\"])\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9b4d3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:02:58.659956Z",
     "iopub.status.busy": "2026-01-28T17:02:58.659420Z",
     "iopub.status.idle": "2026-01-28T17:02:59.667694Z",
     "shell.execute_reply": "2026-01-28T17:02:59.667025Z"
    },
    "id": "Q5F7brdT7h4c",
    "outputId": "09b1bbd0-e2b5-4329-deca-067b497df4de",
    "papermill": {
     "duration": 1.014306,
     "end_time": "2026-01-28T17:02:59.669060",
     "exception": false,
     "start_time": "2026-01-28T17:02:58.654754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fee5a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:02:59.674219Z",
     "iopub.status.busy": "2026-01-28T17:02:59.673963Z",
     "iopub.status.idle": "2026-01-28T17:03:02.522554Z",
     "shell.execute_reply": "2026-01-28T17:03:02.522004Z"
    },
    "id": "pVnEnZikzkpI",
    "outputId": "582eda96-a4f6-4b93-da21-50689b67e962",
    "papermill": {
     "duration": 2.85303,
     "end_time": "2026-01-28T17:03:02.524170",
     "exception": false,
     "start_time": "2026-01-28T17:02:59.671140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n",
      "       \n",
      "root\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n",
      "       \n",
      "StructType([StructField('dept_no', StringType(), True), StructField('dept_name', StringType(), True)])\n",
      "       \n",
      "['dept_no', 'dept_name']\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "+-------+-------+---------+\n",
      "|summary|dept_no|dept_name|\n",
      "+-------+-------+---------+\n",
      "|  count|      9|        9|\n",
      "+-------+-------+---------+\n",
      "\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    25%|   NULL|            NULL|\n",
      "|    50%|   NULL|            NULL|\n",
      "|    75%|   NULL|            NULL|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development')]\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development'), Row(dept_no='d006', dept_name='Quality Management'), Row(dept_no='d007', dept_name='Sales'), Row(dept_no='d008', dept_name='Research'), Row(dept_no='d009', dept_name='Customer Service')]\n",
      "       \n",
      "+-------+---------------+\n",
      "|dept_no|dept_name      |\n",
      "+-------+---------------+\n",
      "|d001   |Marketing      |\n",
      "|d002   |Finance        |\n",
      "|d003   |Human Resources|\n",
      "+-------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "       \n",
      "-RECORD 0--------------------\n",
      " dept_no   | d001            \n",
      " dept_name | Marketing       \n",
      "-RECORD 1--------------------\n",
      " dept_no   | d002            \n",
      " dept_name | Finance         \n",
      "-RECORD 2--------------------\n",
      " dept_no   | d003            \n",
      " dept_name | Human Resources \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'n=3 means it will show top 3 records,\\ntruncate = false it will show complete data of each columns or truncate = 2 it will show first 25 characters of each columns\\nvertical=True data prints in vertically '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=spark.read.csv(\"/content/dept_manager.csv\",header=True,inferSchema=True)\n",
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True,inferSchema=True)\n",
    "print(df.dtypes) # Column names + data types\n",
    "print(\"       \")\n",
    "df.printSchema() # Print full schema\n",
    "print(\"       \")\n",
    "print(df.schema) # Full schema object\n",
    "print(\"       \")\n",
    "print(df.columns) # Column names\n",
    "print(\"       \")\n",
    "df.describe().show() # count, mean, stddev, min, max\n",
    "print(\"       \")\n",
    "df.summary(\"count\").show() # More flexible than describe\n",
    "print(\"       \")\n",
    "df.summary().show() # More flexible than describe\n",
    "print(\"       \")\n",
    "print(df.take(5)) # Returns only the first N rows\n",
    "print(\"       \")\n",
    "print(df.collect()) #Collects ALL rows from ALL partitions & Brings entire dataset into driver memory\n",
    "print(\"       \")\n",
    "##In PySpark:\n",
    "##df.head(5) --> print(df.head(5)) or (for i in df.head(5): print(i))returns a list of Row objects\n",
    "##df.tail(5) returns a list of Row objects\n",
    "##df.count() --> print(df.count()) returns a number\n",
    "##But they donâ€™t automatically print unless you explicitly tell Python to print them.\n",
    "\n",
    "df.show(n=3,truncate=False)\n",
    "print(\"       \")\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "\n",
    "'''n=3 means it will show top 3 records,\n",
    "truncate = false it will show complete data of each columns or truncate = 2 it will show first 25 characters of each columns\n",
    "vertical=True data prints in vertically '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9934e144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:03:02.533475Z",
     "iopub.status.busy": "2026-01-28T17:03:02.533154Z",
     "iopub.status.idle": "2026-01-28T17:03:02.858002Z",
     "shell.execute_reply": "2026-01-28T17:03:02.857460Z"
    },
    "id": "OdRHuR28x8PY",
    "outputId": "ed02bf44-37a1-4966-b5ba-a893fe5fda06",
    "papermill": {
     "duration": 0.332387,
     "end_time": "2026-01-28T17:03:02.860517",
     "exception": false,
     "start_time": "2026-01-28T17:03:02.528130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_no|dept_name|\n",
      "+-------+---------+\n",
      "|d001   |Marketing|\n",
      "|d002   |Finance  |\n",
      "+-------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True,inferSchema=True)\n",
    "department.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaab4080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:03:02.868892Z",
     "iopub.status.busy": "2026-01-28T17:03:02.868545Z",
     "iopub.status.idle": "2026-01-28T17:03:04.648012Z",
     "shell.execute_reply": "2026-01-28T17:03:04.647499Z"
    },
    "papermill": {
     "duration": 1.785687,
     "end_time": "2026-01-28T17:03:04.649735",
     "exception": false,
     "start_time": "2026-01-28T17:03:02.864048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|10001 |d005   |1986-06-26|9999-01-01|\n",
      "|10002 |d007   |1996-08-03|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dep_employee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/dept_emp.csv\",header=True,inferSchema=True)\n",
    "dep_employee.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e41b1e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:03:04.658846Z",
     "iopub.status.busy": "2026-01-28T17:03:04.658539Z",
     "iopub.status.idle": "2026-01-28T17:03:04.987774Z",
     "shell.execute_reply": "2026-01-28T17:03:04.986311Z"
    },
    "papermill": {
     "duration": 0.33568,
     "end_time": "2026-01-28T17:03:04.989404",
     "exception": false,
     "start_time": "2026-01-28T17:03:04.653724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|110022|d001   |1985-01-01|1991-10-01|\n",
      "|110039|d001   |1991-10-01|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dep_manager = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/dept_manager.csv\",header=True,inferSchema=True)\n",
    "dep_manager.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4051ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:03:04.998696Z",
     "iopub.status.busy": "2026-01-28T17:03:04.998367Z",
     "iopub.status.idle": "2026-01-28T17:03:06.150906Z",
     "shell.execute_reply": "2026-01-28T17:03:06.150284Z"
    },
    "papermill": {
     "duration": 1.159348,
     "end_time": "2026-01-28T17:03:06.152676",
     "exception": false,
     "start_time": "2026-01-28T17:03:04.993328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|22393 |1956-06-13|Yongdong  |Kranzdorf|F     |1993-11-02|\n",
      "|22394 |1952-06-01|Katsuo    |Ladret   |F     |1990-04-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empolyee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/employee.csv\",header=True,inferSchema=True)\n",
    "empolyee.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036f1daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:03:06.158891Z",
     "iopub.status.busy": "2026-01-28T17:03:06.158633Z",
     "iopub.status.idle": "2026-01-28T17:03:09.476638Z",
     "shell.execute_reply": "2026-01-28T17:03:09.475714Z"
    },
    "papermill": {
     "duration": 3.323741,
     "end_time": "2026-01-28T17:03:09.479092",
     "exception": false,
     "start_time": "2026-01-28T17:03:06.155351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|amount|from_date |to_date   |\n",
      "+------+------+----------+----------+\n",
      "|201772|54369 |1999-11-26|2000-11-25|\n",
      "|201772|57497 |2000-11-25|2001-11-25|\n",
      "+------+------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salary = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/salary.csv\",header=True,inferSchema=True)\n",
    "salary.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521e687a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:03:09.490406Z",
     "iopub.status.busy": "2026-01-28T17:03:09.490104Z",
     "iopub.status.idle": "2026-01-28T17:03:10.279445Z",
     "shell.execute_reply": "2026-01-28T17:03:10.278819Z"
    },
    "papermill": {
     "duration": 0.796447,
     "end_time": "2026-01-28T17:03:10.280722",
     "exception": false,
     "start_time": "2026-01-28T17:03:09.484275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+----------+\n",
      "|emp_no|title          |from_date |to_date   |\n",
      "+------+---------------+----------+----------+\n",
      "|10001 |Senior Engineer|1986-06-26|9999-01-01|\n",
      "|10002 |Staff          |1996-08-03|9999-01-01|\n",
      "+------+---------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/title.csv\",header=True,inferSchema=True)\n",
    "title.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d83c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T17:03:10.287806Z",
     "iopub.status.busy": "2026-01-28T17:03:10.287567Z",
     "iopub.status.idle": "2026-01-28T17:03:10.290711Z",
     "shell.execute_reply": "2026-01-28T17:03:10.290272Z"
    },
    "id": "DpAbapURx8Cp",
    "papermill": {
     "duration": 0.008173,
     "end_time": "2026-01-28T17:03:10.291938",
     "exception": false,
     "start_time": "2026-01-28T17:03:10.283765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPipMFwqcXYdhRdYCw/6Cxq",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9361375,
     "sourceId": 14654055,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.161999,
   "end_time": "2026-01-28T17:03:12.914083",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-28T17:02:42.752084",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
