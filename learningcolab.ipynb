{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fab4fc",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.002562,
     "end_time": "2026-01-28T16:31:50.271689",
     "exception": false,
     "start_time": "2026-01-28T16:31:50.269127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bharggav/Learning/blob/release1.1/learningcolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ba5d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T16:31:50.276524Z",
     "iopub.status.busy": "2026-01-28T16:31:50.276139Z",
     "iopub.status.idle": "2026-01-28T16:32:08.214643Z",
     "shell.execute_reply": "2026-01-28T16:32:08.213376Z"
    },
    "id": "1_Oj4kqybESl",
    "outputId": "051fabf4-d1e4-477c-f1be-bc8a45da6657",
    "papermill": {
     "duration": 17.943846,
     "end_time": "2026-01-28T16:32:08.217242",
     "exception": false,
     "start_time": "2026-01-28T16:31:50.273396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/28 16:31:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|Bharggav| 27|\n",
      "|  2|  harika| 25|\n",
      "|  3|   Nitya| 27|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"sample data frame\").getOrCreate()\n",
    "a=[(1,\"Bharggav\",\"27\"),(2,\"harika\",\"25\"),(3,\"Nitya\",\"27\")]\n",
    "df=spark.createDataFrame(a,[\"id\",\"name\",\"age\"])\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5de0bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T16:32:08.224896Z",
     "iopub.status.busy": "2026-01-28T16:32:08.224322Z",
     "iopub.status.idle": "2026-01-28T16:32:09.544683Z",
     "shell.execute_reply": "2026-01-28T16:32:09.543542Z"
    },
    "id": "Q5F7brdT7h4c",
    "outputId": "09b1bbd0-e2b5-4329-deca-067b497df4de",
    "papermill": {
     "duration": 1.327174,
     "end_time": "2026-01-28T16:32:09.547487",
     "exception": false,
     "start_time": "2026-01-28T16:32:08.220313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb2ed65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T16:32:09.555798Z",
     "iopub.status.busy": "2026-01-28T16:32:09.555387Z",
     "iopub.status.idle": "2026-01-28T16:32:13.665518Z",
     "shell.execute_reply": "2026-01-28T16:32:13.664515Z"
    },
    "id": "pVnEnZikzkpI",
    "outputId": "582eda96-a4f6-4b93-da21-50689b67e962",
    "papermill": {
     "duration": 4.117406,
     "end_time": "2026-01-28T16:32:13.668189",
     "exception": false,
     "start_time": "2026-01-28T16:32:09.550783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n",
      "       \n",
      "root\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n",
      "       \n",
      "StructType([StructField('dept_no', StringType(), True), StructField('dept_name', StringType(), True)])\n",
      "       \n",
      "['dept_no', 'dept_name']\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "+-------+-------+---------+\n",
      "|summary|dept_no|dept_name|\n",
      "+-------+-------+---------+\n",
      "|  count|      9|        9|\n",
      "+-------+-------+---------+\n",
      "\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    25%|   NULL|            NULL|\n",
      "|    50%|   NULL|            NULL|\n",
      "|    75%|   NULL|            NULL|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development')]\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development'), Row(dept_no='d006', dept_name='Quality Management'), Row(dept_no='d007', dept_name='Sales'), Row(dept_no='d008', dept_name='Research'), Row(dept_no='d009', dept_name='Customer Service')]\n",
      "       \n",
      "+-------+---------------+\n",
      "|dept_no|dept_name      |\n",
      "+-------+---------------+\n",
      "|d001   |Marketing      |\n",
      "|d002   |Finance        |\n",
      "|d003   |Human Resources|\n",
      "+-------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "       \n",
      "-RECORD 0--------------------\n",
      " dept_no   | d001            \n",
      " dept_name | Marketing       \n",
      "-RECORD 1--------------------\n",
      " dept_no   | d002            \n",
      " dept_name | Finance         \n",
      "-RECORD 2--------------------\n",
      " dept_no   | d003            \n",
      " dept_name | Human Resources \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df=spark.read.csv(\"/content/dept_manager.csv\",header=True,inferSchema=True)\n",
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True,inferSchema=True)\n",
    "print(df.dtypes) # Column names + data types\n",
    "print(\"       \")\n",
    "df.printSchema() # Print full schema\n",
    "print(\"       \")\n",
    "print(df.schema) # Full schema object\n",
    "print(\"       \")\n",
    "print(df.columns) # Column names\n",
    "print(\"       \")\n",
    "df.describe().show() # count, mean, stddev, min, max\n",
    "print(\"       \")\n",
    "df.summary(\"count\").show() # More flexible than describe\n",
    "print(\"       \")\n",
    "df.summary().show() # More flexible than describe\n",
    "print(\"       \")\n",
    "print(df.take(5)) # Returns only the first N rows\n",
    "print(\"       \")\n",
    "print(df.collect()) #Collects ALL rows from ALL partitions & Brings entire dataset into driver memory\n",
    "print(\"       \")\n",
    "##In PySpark:\n",
    "##df.head(5) --> print(df.head(5)) or (for i in df.head(5): print(i))returns a list of Row objects\n",
    "##df.tail(5) returns a list of Row objects\n",
    "##df.count() --> print(df.count()) returns a number\n",
    "##But they donâ€™t automatically print unless you explicitly tell Python to print them.\n",
    "\n",
    "df.show(n=3,truncate=False)\n",
    "print(\"       \")\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "\n",
    "'''n=3 means it will show top 3 records,\n",
    "truncate = false it will show complete data of each columns or truncate = 2 it will show first 25 characters of each columns\n",
    "vertical=True data prints in vertically '''\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5aa36",
   "metadata": {
    "id": "OdRHuR28x8PY",
    "outputId": "ed02bf44-37a1-4966-b5ba-a893fe5fda06",
    "papermill": {
     "duration": 0.003357,
     "end_time": "2026-01-28T16:32:13.675207",
     "exception": false,
     "start_time": "2026-01-28T16:32:13.671850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902085a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T16:32:13.684008Z",
     "iopub.status.busy": "2026-01-28T16:32:13.683552Z",
     "iopub.status.idle": "2026-01-28T16:32:13.694177Z",
     "shell.execute_reply": "2026-01-28T16:32:13.692993Z"
    },
    "id": "DpAbapURx8Cp",
    "papermill": {
     "duration": 0.018418,
     "end_time": "2026-01-28T16:32:13.696876",
     "exception": false,
     "start_time": "2026-01-28T16:32:13.678458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPipMFwqcXYdhRdYCw/6Cxq",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9361375,
     "sourceId": 14654055,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.689999,
   "end_time": "2026-01-28T16:32:16.320067",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-28T16:31:46.630068",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
