{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d791e8",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.003378,
     "end_time": "2026-01-28T18:26:40.546543",
     "exception": false,
     "start_time": "2026-01-28T18:26:40.543165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bharggav/Learning/blob/release1.1/learningcolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e1dfd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:26:40.553615Z",
     "iopub.status.busy": "2026-01-28T18:26:40.552846Z",
     "iopub.status.idle": "2026-01-28T18:26:58.299130Z",
     "shell.execute_reply": "2026-01-28T18:26:58.297714Z"
    },
    "id": "1_Oj4kqybESl",
    "outputId": "051fabf4-d1e4-477c-f1be-bc8a45da6657",
    "papermill": {
     "duration": 17.752259,
     "end_time": "2026-01-28T18:26:58.301485",
     "exception": false,
     "start_time": "2026-01-28T18:26:40.549226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/28 18:26:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|Bharggav| 27|\n",
      "|  2|  harika| 25|\n",
      "|  3|   Nitya| 27|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"sample data frame\").getOrCreate()\n",
    "a=[(1,\"Bharggav\",\"27\"),(2,\"harika\",\"25\"),(3,\"Nitya\",\"27\")]\n",
    "df=spark.createDataFrame(a,[\"id\",\"name\",\"age\"])\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb401be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:26:58.314028Z",
     "iopub.status.busy": "2026-01-28T18:26:58.312945Z",
     "iopub.status.idle": "2026-01-28T18:26:59.561534Z",
     "shell.execute_reply": "2026-01-28T18:26:59.560608Z"
    },
    "id": "Q5F7brdT7h4c",
    "outputId": "09b1bbd0-e2b5-4329-deca-067b497df4de",
    "papermill": {
     "duration": 1.257028,
     "end_time": "2026-01-28T18:26:59.563522",
     "exception": false,
     "start_time": "2026-01-28T18:26:58.306494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af276b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:26:59.571588Z",
     "iopub.status.busy": "2026-01-28T18:26:59.570809Z",
     "iopub.status.idle": "2026-01-28T18:27:03.373451Z",
     "shell.execute_reply": "2026-01-28T18:27:03.372461Z"
    },
    "id": "pVnEnZikzkpI",
    "outputId": "582eda96-a4f6-4b93-da21-50689b67e962",
    "papermill": {
     "duration": 3.809201,
     "end_time": "2026-01-28T18:27:03.375643",
     "exception": false,
     "start_time": "2026-01-28T18:26:59.566442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n",
      "       \n",
      "root\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n",
      "       \n",
      "StructType([StructField('dept_no', StringType(), True), StructField('dept_name', StringType(), True)])\n",
      "       \n",
      "['dept_no', 'dept_name']\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "+-------+-------+---------+\n",
      "|summary|dept_no|dept_name|\n",
      "+-------+-------+---------+\n",
      "|  count|      9|        9|\n",
      "+-------+-------+---------+\n",
      "\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    25%|   NULL|            NULL|\n",
      "|    50%|   NULL|            NULL|\n",
      "|    75%|   NULL|            NULL|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development')]\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development'), Row(dept_no='d006', dept_name='Quality Management'), Row(dept_no='d007', dept_name='Sales'), Row(dept_no='d008', dept_name='Research'), Row(dept_no='d009', dept_name='Customer Service')]\n",
      "       \n",
      "+-------+---------------+\n",
      "|dept_no|dept_name      |\n",
      "+-------+---------------+\n",
      "|d001   |Marketing      |\n",
      "|d002   |Finance        |\n",
      "|d003   |Human Resources|\n",
      "+-------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "       \n",
      "-RECORD 0--------------------\n",
      " dept_no   | d001            \n",
      " dept_name | Marketing       \n",
      "-RECORD 1--------------------\n",
      " dept_no   | d002            \n",
      " dept_name | Finance         \n",
      "-RECORD 2--------------------\n",
      " dept_no   | d003            \n",
      " dept_name | Human Resources \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nn=3 means it will show top 3 records,\\ntruncate = false it will show complete data of each columns or truncate = 2 it will show first 25 characters of each columns\\nvertical=True data prints in vertically\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=spark.read.csv(\"/content/dept_manager.csv\",header=True,inferSchema=True)\n",
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True,inferSchema=True)\n",
    "print(df.dtypes) # Column names + data types\n",
    "print(\"       \")\n",
    "df.printSchema() # Print full schema\n",
    "print(\"       \")\n",
    "print(df.schema) # Full schema object\n",
    "print(\"       \")\n",
    "print(df.columns) # Column names\n",
    "print(\"       \")\n",
    "df.describe().show() # count, mean, stddev, min, max\n",
    "print(\"       \")\n",
    "df.summary(\"count\").show() # More flexible than describe\n",
    "print(\"       \")\n",
    "df.summary().show() # More flexible than describe\n",
    "print(\"       \")\n",
    "print(df.take(5)) # Returns only the first N rows\n",
    "print(\"       \")\n",
    "print(df.collect()) #Collects ALL rows from ALL partitions & Brings entire dataset into driver memory\n",
    "print(\"       \")\n",
    "##In PySpark:\n",
    "##df.head(5) --> print(df.head(5)) or (for i in df.head(5): print(i))returns a list of Row objects\n",
    "##df.tail(5) returns a list of Row objects\n",
    "##df.count() --> print(df.count()) returns a number\n",
    "##But they donâ€™t automatically print unless you explicitly tell Python to print them.\n",
    "\n",
    "df.show(n=3,truncate=False)\n",
    "print(\"       \")\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "\n",
    "\"\"\"\n",
    "n=3 means it will show top 3 records,\n",
    "truncate = false it will show complete data of each columns or truncate = 2 it will show first 25 characters of each columns\n",
    "vertical=True data prints in vertically\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5a1b47a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:03.385253Z",
     "iopub.status.busy": "2026-01-28T18:27:03.384682Z",
     "iopub.status.idle": "2026-01-28T18:27:09.721255Z",
     "shell.execute_reply": "2026-01-28T18:27:09.720102Z"
    },
    "papermill": {
     "duration": 6.343909,
     "end_time": "2026-01-28T18:27:09.723329",
     "exception": false,
     "start_time": "2026-01-28T18:27:03.379420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6.120842218399048 seconds\n",
      "Execution time: 0.2090892791748047 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "employee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/salary.csv\", header=True, inferSchema=True)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "employee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/salary.csv\", header=True)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d354bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:09.732347Z",
     "iopub.status.busy": "2026-01-28T18:27:09.731899Z",
     "iopub.status.idle": "2026-01-28T18:27:10.043633Z",
     "shell.execute_reply": "2026-01-28T18:27:10.042672Z"
    },
    "id": "OdRHuR28x8PY",
    "outputId": "ed02bf44-37a1-4966-b5ba-a893fe5fda06",
    "papermill": {
     "duration": 0.318762,
     "end_time": "2026-01-28T18:27:10.045827",
     "exception": false,
     "start_time": "2026-01-28T18:27:09.727065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_no|dept_name|\n",
      "+-------+---------+\n",
      "|d001   |Marketing|\n",
      "|d002   |Finance  |\n",
      "+-------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True,inferSchema=True)\n",
    "department.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ed560b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:10.055015Z",
     "iopub.status.busy": "2026-01-28T18:27:10.054362Z",
     "iopub.status.idle": "2026-01-28T18:27:11.346191Z",
     "shell.execute_reply": "2026-01-28T18:27:11.345455Z"
    },
    "papermill": {
     "duration": 1.300115,
     "end_time": "2026-01-28T18:27:11.349693",
     "exception": false,
     "start_time": "2026-01-28T18:27:10.049578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|10001 |d005   |1986-06-26|9999-01-01|\n",
      "|10002 |d007   |1996-08-03|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dep_employee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/dept_emp.csv\",header=True,inferSchema=True)\n",
    "dep_employee.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d145fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:11.364235Z",
     "iopub.status.busy": "2026-01-28T18:27:11.363820Z",
     "iopub.status.idle": "2026-01-28T18:27:11.733257Z",
     "shell.execute_reply": "2026-01-28T18:27:11.732355Z"
    },
    "papermill": {
     "duration": 0.379614,
     "end_time": "2026-01-28T18:27:11.735604",
     "exception": false,
     "start_time": "2026-01-28T18:27:11.355990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|110022|d001   |1985-01-01|1991-10-01|\n",
      "|110039|d001   |1991-10-01|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dep_manager = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/dept_manager.csv\",header=True,inferSchema=True)\n",
    "dep_manager.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aea04fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:11.750906Z",
     "iopub.status.busy": "2026-01-28T18:27:11.749954Z",
     "iopub.status.idle": "2026-01-28T18:27:12.768425Z",
     "shell.execute_reply": "2026-01-28T18:27:12.767088Z"
    },
    "papermill": {
     "duration": 1.028462,
     "end_time": "2026-01-28T18:27:12.770580",
     "exception": false,
     "start_time": "2026-01-28T18:27:11.742118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|22393 |1956-06-13|Yongdong  |Kranzdorf|F     |1993-11-02|\n",
      "|22394 |1952-06-01|Katsuo    |Ladret   |F     |1990-04-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empolyee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/employee.csv\",header=True,inferSchema=True)\n",
    "empolyee.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18aff9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:12.780864Z",
     "iopub.status.busy": "2026-01-28T18:27:12.780013Z",
     "iopub.status.idle": "2026-01-28T18:27:17.309914Z",
     "shell.execute_reply": "2026-01-28T18:27:17.308890Z"
    },
    "papermill": {
     "duration": 4.538787,
     "end_time": "2026-01-28T18:27:17.313513",
     "exception": false,
     "start_time": "2026-01-28T18:27:12.774726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|amount|from_date |to_date   |\n",
      "+------+------+----------+----------+\n",
      "|201772|54369 |1999-11-26|2000-11-25|\n",
      "|201772|57497 |2000-11-25|2001-11-25|\n",
      "+------+------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "salary = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/salary.csv\",header=True,inferSchema=True)\n",
    "salary.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f9028d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:17.329306Z",
     "iopub.status.busy": "2026-01-28T18:27:17.328871Z",
     "iopub.status.idle": "2026-01-28T18:27:18.443779Z",
     "shell.execute_reply": "2026-01-28T18:27:18.443003Z"
    },
    "papermill": {
     "duration": 1.125667,
     "end_time": "2026-01-28T18:27:18.446379",
     "exception": false,
     "start_time": "2026-01-28T18:27:17.320712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+----------+\n",
      "|emp_no|title          |from_date |to_date   |\n",
      "+------+---------------+----------+----------+\n",
      "|10001 |Senior Engineer|1986-06-26|9999-01-01|\n",
      "|10002 |Staff          |1996-08-03|9999-01-01|\n",
      "+------+---------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/title.csv\",header=True,inferSchema=True)\n",
    "title.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5d3f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:18.463573Z",
     "iopub.status.busy": "2026-01-28T18:27:18.462933Z",
     "iopub.status.idle": "2026-01-28T18:27:19.388924Z",
     "shell.execute_reply": "2026-01-28T18:27:19.387897Z"
    },
    "papermill": {
     "duration": 0.937756,
     "end_time": "2026-01-28T18:27:19.391876",
     "exception": false,
     "start_time": "2026-01-28T18:27:18.454120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we want to update specific datatype null values to defaults EX:- int=0,string=Unknown.\n",
      "+-----+---+------+------------------+----------+\n",
      "| Name| ID|Salary|        Department|       doj|\n",
      "+-----+---+------+------------------+----------+\n",
      "|Alice|  1| 55000|                HR|2025-01-12|\n",
      "|  Bob|  2| 60000|           Finance|2022-11-23|\n",
      "| NULL|  3| 70000|Software_Developer|2020-06-19|\n",
      "|  Dey|  4|  NULL|             GenAI|2026-01-15|\n",
      "+-----+---+------+------------------+----------+\n",
      "\n",
      "{'Name': 'Unknown', 'ID': 0, 'Salary': 0, 'Department': 'Unknown', 'doj': 'Unknown'}\n",
      "+-------+---+------+------------------+----------+\n",
      "|   Name| ID|Salary|        Department|       doj|\n",
      "+-------+---+------+------------------+----------+\n",
      "|  Alice|  1| 55000|                HR|2025-01-12|\n",
      "|    Bob|  2| 60000|           Finance|2022-11-23|\n",
      "|Unknown|  3| 70000|Software_Developer|2020-06-19|\n",
      "|    Dey|  4|     0|             GenAI|2026-01-15|\n",
      "+-------+---+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"If we want to update specific datatype null values to defaults EX:- int=0,string=Unknown.\")\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"Alice\", 1,55000,\"HR\",\"2025-01-12\"), (\"Bob\", 2,60000,\"Finance\",\"2022-11-23\"),\n",
    "        (None,3,70000,'Software_Developer',\"2020-06-19\"),(\"Dey\",4,None,\"GenAI\",\"2026-01-15\")]\n",
    "columns = [\"Name\", \"ID\", \"Salary\",\"Department\",\"doj\"]\n",
    "emp = spark.createDataFrame(data, columns)\n",
    "emp.show()\n",
    "fill_dict = {}\n",
    "for col, dtype in emp.dtypes:\n",
    "    if dtype in (\"int\", \"bigint\"):\n",
    "        fill_dict[col] = 0\n",
    "    elif dtype == \"string\":\n",
    "        fill_dict[col] = \"Unknown\"\n",
    "    else:\n",
    "        print(f\"Unhandled data type: {i[1]} for column: {i[0]}\")\n",
    "print(fill_dict)\n",
    "emp = emp.fillna(fill_dict)\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b1b8fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:19.410015Z",
     "iopub.status.busy": "2026-01-28T18:27:19.408811Z",
     "iopub.status.idle": "2026-01-28T18:27:20.233064Z",
     "shell.execute_reply": "2026-01-28T18:27:20.230699Z"
    },
    "papermill": {
     "duration": 0.835677,
     "end_time": "2026-01-28T18:27:20.235446",
     "exception": false,
     "start_time": "2026-01-28T18:27:19.399769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+------------------+----------+\n",
      "| Name| ID|Salary|        Department|       doj|\n",
      "+-----+---+------+------------------+----------+\n",
      "|Alice|  1| 55000|                HR|2025-01-12|\n",
      "|  Bob|  2| 60000|           Finance|2022-11-23|\n",
      "| NULL|  3| 70000|Software_Developer|2020-06-19|\n",
      "|  Dey|  4|  NULL|             GenAI|2026-01-15|\n",
      "+-----+---+------+------------------+----------+\n",
      "\n",
      "+-------+---+------+------------------+----------+\n",
      "|   Name| ID|Salary|        Department|       doj|\n",
      "+-------+---+------+------------------+----------+\n",
      "|  Alice|  1| 55000|                HR|2025-01-12|\n",
      "|    Bob|  2| 60000|           Finance|2022-11-23|\n",
      "|Unknown|  3| 70000|Software_Developer|2020-06-19|\n",
      "|    Dey|  4|     0|             GenAI|2026-01-15|\n",
      "+-------+---+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 1,55000,\"HR\",\"2025-01-12\"), (\"Bob\", 2,60000,\"Finance\",\"2022-11-23\"),\n",
    "        (None,3,70000,'Software_Developer',\"2020-06-19\"),(\"Dey\",4,None,\"GenAI\",\"2026-01-15\")]\n",
    "columns = [\"Name\", \"ID\", \"Salary\",\"Department\",\"doj\"]\n",
    "emp = spark.createDataFrame(data, columns)\n",
    "emp.show()\n",
    "\"\"\"\n",
    "emp=emp.fillna(value=0,subset=[\"Salary\"])\n",
    "emp=emp.fillna(0,[\"Salary\"])\n",
    "emp = emp.fillna(0, [\"Salary\", \"ID\"])          # OK (numeric)\n",
    "emp = emp.fillna(\"Unknown\", [\"Name\"])          # OK (string)\n",
    "emp = emp.fillna(\"Unknown\", [\"Name\",\"Dept\"])   # OK (all string)\n",
    "\"\"\"\n",
    "emp=emp.na.fill({\"Name\": \"Unknown\", \"Salary\": 0})\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b308f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:27:20.252419Z",
     "iopub.status.busy": "2026-01-28T18:27:20.252031Z",
     "iopub.status.idle": "2026-01-28T18:27:21.225285Z",
     "shell.execute_reply": "2026-01-28T18:27:21.224443Z"
    },
    "id": "DpAbapURx8Cp",
    "papermill": {
     "duration": 0.984165,
     "end_time": "2026-01-28T18:27:21.227265",
     "exception": false,
     "start_time": "2026-01-28T18:27:20.243100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPipMFwqcXYdhRdYCw/6Cxq",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9361375,
     "sourceId": 14654055,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46.899183,
   "end_time": "2026-01-28T18:27:23.855188",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-28T18:26:36.956005",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
