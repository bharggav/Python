{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2560e701",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.004186,
     "end_time": "2026-01-28T18:17:20.954876",
     "exception": false,
     "start_time": "2026-01-28T18:17:20.950690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bharggav/Learning/blob/release1.1/learningcolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48445d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:20.963274Z",
     "iopub.status.busy": "2026-01-28T18:17:20.962145Z",
     "iopub.status.idle": "2026-01-28T18:17:38.399402Z",
     "shell.execute_reply": "2026-01-28T18:17:38.398065Z"
    },
    "id": "1_Oj4kqybESl",
    "outputId": "051fabf4-d1e4-477c-f1be-bc8a45da6657",
    "papermill": {
     "duration": 17.443624,
     "end_time": "2026-01-28T18:17:38.401562",
     "exception": false,
     "start_time": "2026-01-28T18:17:20.957938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/28 18:17:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|Bharggav| 27|\n",
      "|  2|  harika| 25|\n",
      "|  3|   Nitya| 27|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"sample data frame\").getOrCreate()\n",
    "a=[(1,\"Bharggav\",\"27\"),(2,\"harika\",\"25\"),(3,\"Nitya\",\"27\")]\n",
    "df=spark.createDataFrame(a,[\"id\",\"name\",\"age\"])\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3234c9d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:38.411210Z",
     "iopub.status.busy": "2026-01-28T18:17:38.410119Z",
     "iopub.status.idle": "2026-01-28T18:17:39.893200Z",
     "shell.execute_reply": "2026-01-28T18:17:39.892212Z"
    },
    "id": "Q5F7brdT7h4c",
    "outputId": "09b1bbd0-e2b5-4329-deca-067b497df4de",
    "papermill": {
     "duration": 1.490951,
     "end_time": "2026-01-28T18:17:39.896204",
     "exception": false,
     "start_time": "2026-01-28T18:17:38.405253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af9eaa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:39.905912Z",
     "iopub.status.busy": "2026-01-28T18:17:39.904932Z",
     "iopub.status.idle": "2026-01-28T18:17:44.287461Z",
     "shell.execute_reply": "2026-01-28T18:17:44.286412Z"
    },
    "id": "pVnEnZikzkpI",
    "outputId": "582eda96-a4f6-4b93-da21-50689b67e962",
    "papermill": {
     "duration": 4.389934,
     "end_time": "2026-01-28T18:17:44.289863",
     "exception": false,
     "start_time": "2026-01-28T18:17:39.899929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dept_no', 'string'), ('dept_name', 'string')]\n",
      "       \n",
      "root\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      "\n",
      "       \n",
      "StructType([StructField('dept_no', StringType(), True), StructField('dept_name', StringType(), True)])\n",
      "       \n",
      "['dept_no', 'dept_name']\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "+-------+-------+---------+\n",
      "|summary|dept_no|dept_name|\n",
      "+-------+-------+---------+\n",
      "|  count|      9|        9|\n",
      "+-------+-------+---------+\n",
      "\n",
      "       \n",
      "+-------+-------+----------------+\n",
      "|summary|dept_no|       dept_name|\n",
      "+-------+-------+----------------+\n",
      "|  count|      9|               9|\n",
      "|   mean|   NULL|            NULL|\n",
      "| stddev|   NULL|            NULL|\n",
      "|    min|   d001|Customer Service|\n",
      "|    25%|   NULL|            NULL|\n",
      "|    50%|   NULL|            NULL|\n",
      "|    75%|   NULL|            NULL|\n",
      "|    max|   d009|           Sales|\n",
      "+-------+-------+----------------+\n",
      "\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development')]\n",
      "       \n",
      "[Row(dept_no='d001', dept_name='Marketing'), Row(dept_no='d002', dept_name='Finance'), Row(dept_no='d003', dept_name='Human Resources'), Row(dept_no='d004', dept_name='Production'), Row(dept_no='d005', dept_name='Development'), Row(dept_no='d006', dept_name='Quality Management'), Row(dept_no='d007', dept_name='Sales'), Row(dept_no='d008', dept_name='Research'), Row(dept_no='d009', dept_name='Customer Service')]\n",
      "       \n",
      "+-------+---------------+\n",
      "|dept_no|dept_name      |\n",
      "+-------+---------------+\n",
      "|d001   |Marketing      |\n",
      "|d002   |Finance        |\n",
      "|d003   |Human Resources|\n",
      "+-------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "       \n",
      "-RECORD 0--------------------\n",
      " dept_no   | d001            \n",
      " dept_name | Marketing       \n",
      "-RECORD 1--------------------\n",
      " dept_no   | d002            \n",
      " dept_name | Finance         \n",
      "-RECORD 2--------------------\n",
      " dept_no   | d003            \n",
      " dept_name | Human Resources \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nn=3 means it will show top 3 records,\\ntruncate = false it will show complete data of each columns or truncate = 2 it will show first 25 characters of each columns\\nvertical=True data prints in vertically\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=spark.read.csv(\"/content/dept_manager.csv\",header=True,inferSchema=True)\n",
    "df=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True,inferSchema=True)\n",
    "print(df.dtypes) # Column names + data types\n",
    "print(\"       \")\n",
    "df.printSchema() # Print full schema\n",
    "print(\"       \")\n",
    "print(df.schema) # Full schema object\n",
    "print(\"       \")\n",
    "print(df.columns) # Column names\n",
    "print(\"       \")\n",
    "df.describe().show() # count, mean, stddev, min, max\n",
    "print(\"       \")\n",
    "df.summary(\"count\").show() # More flexible than describe\n",
    "print(\"       \")\n",
    "df.summary().show() # More flexible than describe\n",
    "print(\"       \")\n",
    "print(df.take(5)) # Returns only the first N rows\n",
    "print(\"       \")\n",
    "print(df.collect()) #Collects ALL rows from ALL partitions & Brings entire dataset into driver memory\n",
    "print(\"       \")\n",
    "##In PySpark:\n",
    "##df.head(5) --> print(df.head(5)) or (for i in df.head(5): print(i))returns a list of Row objects\n",
    "##df.tail(5) returns a list of Row objects\n",
    "##df.count() --> print(df.count()) returns a number\n",
    "##But they donâ€™t automatically print unless you explicitly tell Python to print them.\n",
    "\n",
    "df.show(n=3,truncate=False)\n",
    "print(\"       \")\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "\n",
    "\"\"\"\n",
    "n=3 means it will show top 3 records,\n",
    "truncate = false it will show complete data of each columns or truncate = 2 it will show first 25 characters of each columns\n",
    "vertical=True data prints in vertically\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01acfc6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:44.306151Z",
     "iopub.status.busy": "2026-01-28T18:17:44.305340Z",
     "iopub.status.idle": "2026-01-28T18:17:51.632699Z",
     "shell.execute_reply": "2026-01-28T18:17:51.631483Z"
    },
    "papermill": {
     "duration": 7.338913,
     "end_time": "2026-01-28T18:17:51.635843",
     "exception": false,
     "start_time": "2026-01-28T18:17:44.296930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 7.086377382278442 seconds\n",
      "Execution time: 0.23096132278442383 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "employee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/salary.csv\", header=True, inferSchema=True)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "employee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/salary.csv\", header=True)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef9f9db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:51.652278Z",
     "iopub.status.busy": "2026-01-28T18:17:51.651386Z",
     "iopub.status.idle": "2026-01-28T18:17:52.061280Z",
     "shell.execute_reply": "2026-01-28T18:17:52.060400Z"
    },
    "id": "OdRHuR28x8PY",
    "outputId": "ed02bf44-37a1-4966-b5ba-a893fe5fda06",
    "papermill": {
     "duration": 0.421219,
     "end_time": "2026-01-28T18:17:52.064098",
     "exception": false,
     "start_time": "2026-01-28T18:17:51.642879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_no|dept_name|\n",
      "+-------+---------+\n",
      "|d001   |Marketing|\n",
      "|d002   |Finance  |\n",
      "+-------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department=spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/department.csv\",header=True,inferSchema=True)\n",
    "department.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1766f987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:52.080707Z",
     "iopub.status.busy": "2026-01-28T18:17:52.079767Z",
     "iopub.status.idle": "2026-01-28T18:17:53.787128Z",
     "shell.execute_reply": "2026-01-28T18:17:53.784524Z"
    },
    "papermill": {
     "duration": 1.719781,
     "end_time": "2026-01-28T18:17:53.790892",
     "exception": false,
     "start_time": "2026-01-28T18:17:52.071111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|10001 |d005   |1986-06-26|9999-01-01|\n",
      "|10002 |d007   |1996-08-03|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dep_employee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/dept_emp.csv\",header=True,inferSchema=True)\n",
    "dep_employee.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5b79d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:53.807425Z",
     "iopub.status.busy": "2026-01-28T18:17:53.806954Z",
     "iopub.status.idle": "2026-01-28T18:17:54.212718Z",
     "shell.execute_reply": "2026-01-28T18:17:54.211832Z"
    },
    "papermill": {
     "duration": 0.417132,
     "end_time": "2026-01-28T18:17:54.215237",
     "exception": false,
     "start_time": "2026-01-28T18:17:53.798105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|110022|d001   |1985-01-01|1991-10-01|\n",
      "|110039|d001   |1991-10-01|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dep_manager = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/dept_manager.csv\",header=True,inferSchema=True)\n",
    "dep_manager.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721716e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:54.226749Z",
     "iopub.status.busy": "2026-01-28T18:17:54.225658Z",
     "iopub.status.idle": "2026-01-28T18:17:55.692464Z",
     "shell.execute_reply": "2026-01-28T18:17:55.691385Z"
    },
    "papermill": {
     "duration": 1.476088,
     "end_time": "2026-01-28T18:17:55.695973",
     "exception": false,
     "start_time": "2026-01-28T18:17:54.219885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |\n",
      "+------+----------+----------+---------+------+----------+\n",
      "|22393 |1956-06-13|Yongdong  |Kranzdorf|F     |1993-11-02|\n",
      "|22394 |1952-06-01|Katsuo    |Ladret   |F     |1990-04-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empolyee = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/employee.csv\",header=True,inferSchema=True)\n",
    "empolyee.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f9bd91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:17:55.714907Z",
     "iopub.status.busy": "2026-01-28T18:17:55.713976Z",
     "iopub.status.idle": "2026-01-28T18:18:00.968168Z",
     "shell.execute_reply": "2026-01-28T18:18:00.967226Z"
    },
    "papermill": {
     "duration": 5.266412,
     "end_time": "2026-01-28T18:18:00.970369",
     "exception": false,
     "start_time": "2026-01-28T18:17:55.703957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|amount|from_date |to_date   |\n",
      "+------+------+----------+----------+\n",
      "|201772|54369 |1999-11-26|2000-11-25|\n",
      "|201772|57497 |2000-11-25|2001-11-25|\n",
      "+------+------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/salary.csv\",header=True,inferSchema=True)\n",
    "salary.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf8902a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:18:00.981785Z",
     "iopub.status.busy": "2026-01-28T18:18:00.981413Z",
     "iopub.status.idle": "2026-01-28T18:18:02.205980Z",
     "shell.execute_reply": "2026-01-28T18:18:02.205077Z"
    },
    "papermill": {
     "duration": 1.233315,
     "end_time": "2026-01-28T18:18:02.208665",
     "exception": false,
     "start_time": "2026-01-28T18:18:00.975350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+----------+\n",
      "|emp_no|title          |from_date |to_date   |\n",
      "+------+---------------+----------+----------+\n",
      "|10001 |Senior Engineer|1986-06-26|9999-01-01|\n",
      "|10002 |Staff          |1996-08-03|9999-01-01|\n",
      "+------+---------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "title = spark.read.csv(\"/kaggle/input/learning1/Pyspark/data_sets/title.csv\",header=True,inferSchema=True)\n",
    "title.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf53794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:18:02.220565Z",
     "iopub.status.busy": "2026-01-28T18:18:02.220227Z",
     "iopub.status.idle": "2026-01-28T18:18:03.220283Z",
     "shell.execute_reply": "2026-01-28T18:18:03.219444Z"
    },
    "papermill": {
     "duration": 1.008275,
     "end_time": "2026-01-28T18:18:03.222231",
     "exception": false,
     "start_time": "2026-01-28T18:18:02.213956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we want to update specific datatype null values to defaults EX:- int=0,string=Unknown.\n",
      "+-----+---+------+------------------+----------+\n",
      "| Name| ID|Salary|        Department|       doj|\n",
      "+-----+---+------+------------------+----------+\n",
      "|Alice|  1| 55000|                HR|2025-01-12|\n",
      "|  Bob|  2| 60000|           Finance|2022-11-23|\n",
      "| NULL|  3| 70000|Software_Developer|2020-06-19|\n",
      "|  Dey|  4|  NULL|             GenAI|2026-01-15|\n",
      "+-----+---+------+------------------+----------+\n",
      "\n",
      "{'Name': 'Unknown', 'ID': 0, 'Salary': 0, 'Department': 'Unknown', 'doj': 'Unknown'}\n",
      "+-------+---+------+------------------+----------+\n",
      "|   Name| ID|Salary|        Department|       doj|\n",
      "+-------+---+------+------------------+----------+\n",
      "|  Alice|  1| 55000|                HR|2025-01-12|\n",
      "|    Bob|  2| 60000|           Finance|2022-11-23|\n",
      "|Unknown|  3| 70000|Software_Developer|2020-06-19|\n",
      "|    Dey|  4|     0|             GenAI|2026-01-15|\n",
      "+-------+---+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"If we want to update specific datatype null values to defaults EX:- int=0,string=Unknown.\")\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"Alice\", 1,55000,\"HR\",\"2025-01-12\"), (\"Bob\", 2,60000,\"Finance\",\"2022-11-23\"),\n",
    "        (None,3,70000,'Software_Developer',\"2020-06-19\"),(\"Dey\",4,None,\"GenAI\",\"2026-01-15\")]\n",
    "columns = [\"Name\", \"ID\", \"Salary\",\"Department\",\"doj\"]\n",
    "emp = spark.createDataFrame(data, columns)\n",
    "emp.show()\n",
    "fill_dict = {}\n",
    "for col, dtype in emp.dtypes:\n",
    "    if dtype in (\"int\", \"bigint\"):\n",
    "        fill_dict[col] = 0\n",
    "    elif dtype == \"string\":\n",
    "        fill_dict[col] = \"Unknown\"\n",
    "    else:\n",
    "        print(f\"Unhandled data type: {i[1]} for column: {i[0]}\")\n",
    "print(fill_dict)\n",
    "emp = emp.fillna(fill_dict)\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90354dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:18:03.234432Z",
     "iopub.status.busy": "2026-01-28T18:18:03.233992Z",
     "iopub.status.idle": "2026-01-28T18:18:04.080421Z",
     "shell.execute_reply": "2026-01-28T18:18:04.079589Z"
    },
    "papermill": {
     "duration": 0.856186,
     "end_time": "2026-01-28T18:18:04.083385",
     "exception": false,
     "start_time": "2026-01-28T18:18:03.227199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+------------------+----------+\n",
      "| Name| ID|Salary|        Department|       doj|\n",
      "+-----+---+------+------------------+----------+\n",
      "|Alice|  1| 55000|                HR|2025-01-12|\n",
      "|  Bob|  2| 60000|           Finance|2022-11-23|\n",
      "| NULL|  3| 70000|Software_Developer|2020-06-19|\n",
      "|  Dey|  4|  NULL|             GenAI|2026-01-15|\n",
      "+-----+---+------+------------------+----------+\n",
      "\n",
      "+-------+---+------+------------------+----------+\n",
      "|   Name| ID|Salary|        Department|       doj|\n",
      "+-------+---+------+------------------+----------+\n",
      "|  Alice|  1| 55000|                HR|2025-01-12|\n",
      "|    Bob|  2| 60000|           Finance|2022-11-23|\n",
      "|Unknown|  3| 70000|Software_Developer|2020-06-19|\n",
      "|    Dey|  4|     0|             GenAI|2026-01-15|\n",
      "+-------+---+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 1,55000,\"HR\",\"2025-01-12\"), (\"Bob\", 2,60000,\"Finance\",\"2022-11-23\"),\n",
    "        (None,3,70000,'Software_Developer',\"2020-06-19\"),(\"Dey\",4,None,\"GenAI\",\"2026-01-15\")]\n",
    "columns = [\"Name\", \"ID\", \"Salary\",\"Department\",\"doj\"]\n",
    "emp = spark.createDataFrame(data, columns)\n",
    "emp.show()\n",
    "\"\"\"\n",
    "emp=emp.fillna(value=0,subset=[\"Salary\"])\n",
    "emp=emp.fillna(0,[\"Salary\"])\n",
    "emp = emp.fillna(0, [\"Salary\", \"ID\"])          # OK (numeric)\n",
    "emp = emp.fillna(\"Unknown\", [\"Name\"])          # OK (string)\n",
    "emp = emp.fillna(\"Unknown\", [\"Name\",\"Dept\"])   # OK (all string)\n",
    "\"\"\"\n",
    "emp=emp.na.fill({\"Name\": \"Unknown\", \"Salary\": 0})\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038c4edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T18:18:04.103159Z",
     "iopub.status.busy": "2026-01-28T18:18:04.102819Z",
     "iopub.status.idle": "2026-01-28T18:18:05.075783Z",
     "shell.execute_reply": "2026-01-28T18:18:05.074669Z"
    },
    "id": "DpAbapURx8Cp",
    "papermill": {
     "duration": 0.985483,
     "end_time": "2026-01-28T18:18:05.078022",
     "exception": false,
     "start_time": "2026-01-28T18:18:04.092539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPipMFwqcXYdhRdYCw/6Cxq",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9361375,
     "sourceId": 14654055,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 49.782407,
   "end_time": "2026-01-28T18:18:07.703236",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-28T18:17:17.920829",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
