{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19f42c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T17:27:35.256541Z",
     "start_time": "2025-10-22T17:27:33.832295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n",
      "HADOOP_HOME: C:\\Program Files\\Hadoop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-11\"\n",
    "# os.environ[\"SPARK_HOME\"] = r\"C:\\Program Files\\spark\"\n",
    "# os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    " \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "import pyspark\n",
    "print(\"Spark Version:\", pyspark.__version__)\n",
    "print(\"HADOOP_HOME:\", os.environ.get(\"HADOOP_HOME\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a56e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T17:28:00.214405Z",
     "start_time": "2025-10-22T17:27:38.397577Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\Datasets-master\\cars.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#df.dtypes\n",
    "#df.printSchema()\n",
    "#df.show(n=5,truncate=False,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c71855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "{'Name': 'string', 'Age': 'bigint'}\n",
      "+-----+---+------+----------+\n",
      "|Name |ID |Salary|Department|\n",
      "+-----+---+------+----------+\n",
      "|Alice|1  |55000 |HR        |\n",
      "|Bob  |2  |60000 |Finance   |\n",
      "+-----+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Postgres to CSV\").getOrCreate()\n",
    "\n",
    "\n",
    "a={\n",
    "    \"Name\":[\"Alice\",\"Bob\"],\n",
    "    \"Age\":[25,30]\n",
    "}\n",
    "\n",
    "df1=pd.DataFrame(a)\n",
    "df=spark.createDataFrame(df1)\n",
    "#df.show()\n",
    "df.printSchema()\n",
    "df.dtypes\n",
    "a=df.dtypes\n",
    "print(dict(a))\n",
    "df.schema\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"Alice\", 1,55000,\"HR\"), (\"Bob\", 2,60000,\"Finance\")]\n",
    "columns = [\"Name\", \"ID\", \"Salary\",\"Department\"]\n",
    "emp = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "emp.show(n=4, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3759a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameSampling\").getOrCreate()\n",
    "\n",
    "employee = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\employee.csv\", header=True, inferSchema=True)\n",
    "# department = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\department.csv\", header=True, inferSchema=True)\n",
    "# dept_emp = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\dept_emp.csv\", header=True, inferSchema=True)\n",
    "# dept_manager = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\dept_manager.csv\", header=True, inferSchema=True)\n",
    "# salaries = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\salary.csv\", header=True, inferSchema=True)\n",
    "# titles = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\title.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# employee.printSchema()\n",
    "# department.printSchema()\n",
    "# dept_emp.printSchema()\n",
    "# dept_manager.printSchema()\n",
    "# salaries.printSchema()\n",
    "# titles.printSchema()\n",
    "\n",
    "#print(employee.dtypes)\n",
    "# print(department.dtypes)\n",
    "# print(dept_emp.dtypes)\n",
    "# print(dept_manager.dtypes)\n",
    "# print(salaries.dtypes)\n",
    "# print(titles.dtypes)\n",
    "# print(titles.columns)\n",
    "\n",
    "# a=employee.dtypes\n",
    "# for i in a:\n",
    "#     if i[1]=='string':\n",
    "#         employee = employee.fillna({i[0]:'Unknown'})\n",
    "#         print(i[1])\n",
    "#     elif i[1]=='int':\n",
    "#         employee = employee.fillna({i[0]:0})\n",
    "#         print(i[1])\n",
    "#     elif i[1]=='date':\n",
    "#         employee = employee.fillna({i[0]:'1900-01-01'})\n",
    "#         print(i[1])     \n",
    "#     elif i[1]=='double':\n",
    "#         employee = employee.fillna({i[0]:0.0})\n",
    "#         print(i[1])\n",
    "#     elif i[1]=='float':\n",
    "#         employee = employee.fillna({i[0]:0.0})\n",
    "#         print(i[1])\n",
    "#     elif i[1]=='long':\n",
    "#         employee = employee.fillna({i[0]:0})\n",
    "#         print(i[1])\n",
    "#     elif i[1]=='boolean':\n",
    "#         employee = employee.fillna({i[0]:False})\n",
    "#         print(i[1])\n",
    "#     else:\n",
    "#         print(f\"Unhandled data type: {i[1]} for column: {i[0]}\")\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d1fe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.5634267330169678 seconds\n",
      "Execution time: 0.11670732498168945 seconds\n",
      "Execution time: 0.6807420253753662 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameSampling\").getOrCreate()\n",
    "import time\n",
    "\n",
    "start_time1 = time.time()\n",
    "start_time = time.time()\n",
    "employee = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\employee.csv\", header=True, inferSchema=True)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "employee = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Python\\Pyspark\\data_sets\\employee.csv\", header=True)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "# employee.filter(employee[\"last_name\"].isNull()).show()\n",
    "# employee.filter(col(\"last_name\").isNull()).show()\n",
    "# employee.where(\"last_name is null\").show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "08fbdcbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T17:42:17.627398Z",
     "start_time": "2025-10-22T17:42:08.564110Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameSampling\").getOrCreate()\n",
    "\n",
    "employee = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Learning\\Pyspark\\data_sets\\employee.csv\", header=True, inferSchema=True)\n",
    "department = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Learning\\Pyspark\\data_sets\\department.csv\", header=True, inferSchema=True)\n",
    "dept_emp = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Learning\\Pyspark\\data_sets\\dept_emp.csv\", header=True, inferSchema=True)\n",
    "dept_manager = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Learning\\Pyspark\\data_sets\\dept_manager.csv\", header=True, inferSchema=True)\n",
    "salaries = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Learning\\Pyspark\\data_sets\\salary.csv\", header=True, inferSchema=True)\n",
    "titles = spark.read.csv(r\"C:\\Users\\bharg\\Documents\\GitHub\\Learning\\Pyspark\\data_sets\\title.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(employee.count())\n",
    "print(department.count())\n",
    "print(dept_emp.count())\n",
    "print(dept_manager.count())\n",
    "print(salaries.count())\n",
    "print(titles.count())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300024\n",
      "9\n",
      "331603\n",
      "24\n",
      "2844047\n",
      "443308\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
